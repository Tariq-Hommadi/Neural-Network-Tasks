{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A4Q6DBB3FVY"
      },
      "source": [
        "# Neural Network Implementation-From Scratch "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ7Bd9ZsoXCR"
      },
      "source": [
        "## A. Neural Network: Binary Classification  \n",
        "\n",
        "In this assignment, we will learn to build a fully-connected neural network with standard architecture, i.e., only one hidden layer. We will use the `Breast cancer wisconsin (diagnostic) dataset` available in `https://scikit-learn.org/stable/datasets/index.html`. It has a total of 569 sample with two classes (Malignant and Benign). Each sample has 30 real-valued features. \n",
        "\n",
        "**This assignment will help you understand and implement:**\n",
        "- A neural network for binary classification consisting of one hidden layer with non-linear activation. You will implement ideas like forward propogation, computing the loss, bagward propogation, and parameter(weights) update. Using the trained network, you will learn to predict a class given the features of a sample.\n",
        "\n",
        "**Note:** There are multiple conventions for coding neural networks. We will follow the conventions suggested by Andrew Ng: https://www.coursera.org/learn/neural-networks-deep-learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqaGHZSJoXCU"
      },
      "outputs": [],
      "source": [
        "# Package imports\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import sklearn.linear_model\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGkbrWCS3FVm"
      },
      "source": [
        "## 1. Loading the dataset and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LENi5Q9Z3FVn",
        "outputId": "86011d56-f447-4041-de9f-cbc28bb0001b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(569, 30)\n"
          ]
        }
      ],
      "source": [
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(X, y, test_size =0.2, random_state=0)\n",
        "\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkzSuMSr3FVo"
      },
      "outputs": [],
      "source": [
        "scaler = preprocessing.StandardScaler().fit(train_data)\n",
        "train_data = scaler.transform(train_data)\n",
        "test_data = scaler.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpzj7FmV3FVq"
      },
      "outputs": [],
      "source": [
        "trainx = train_data.T\n",
        "trainy = train_labels.reshape(-1,1).T\n",
        "\n",
        "testx = test_data.T\n",
        "testy =test_labels.reshape(-1,1).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d78k8Prk3FVs",
        "outputId": "d378198e-c57a-4935-f60b-7f04d94e8157"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((30, 455), (1, 455), (30, 114), (1, 114))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainx.shape, trainy.shape, testx.shape, testy.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dIZ3-k23FVu"
      },
      "outputs": [],
      "source": [
        "X=trainx\n",
        "Y=trainy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFAQt6HYoXCh",
        "outputId": "fcf3d7a0-01ed-4d8f-c798-72c6eb88f4ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No. of training samples: 455\n",
            "Number of features per sample: 30\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "shape_X = trainx.shape\n",
        "shape_Y = trainy.shape\n",
        "m = shape_X[1]  # training set size\n",
        "### END CODE HERE ###\n",
        "\n",
        "print ('No. of training samples: ' + str(m))\n",
        "print ('Number of features per sample: ' + str(shape_X[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2AFzm-hoXCv"
      },
      "source": [
        "## 2 - Neural Network model\n",
        "\n",
        "We will train a Neural Network with a single hidden layer.\n",
        "\n",
        "**Mathematically**:\n",
        "\n",
        "For one example $x^{(i)}$:\n",
        "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$ \n",
        "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
        "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$\n",
        "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
        "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
        "\n",
        "Given the predictions on all the examples, you can also compute the cost (loss) $J$ as follows: \n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
        "\n",
        "**Important**: Building the NN will involve the following:\n",
        "\n",
        "    1. Specify the network structure in terms of the number of input units, number of neurons in the hidden units, ...\n",
        "    2. Initialize the parameters of the model\n",
        "    3. Loop a number of iterations:\n",
        "        - Forward propagation\n",
        "        - Compute loss and the overall loss\n",
        "        - Backward propagation\n",
        "        - Update the parameters (gradient descent)\n",
        "\n",
        "In order to make the code modular, we can implement each of the step as a function and them combine them together to build the overall model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC3UrB-AoXCw"
      },
      "source": [
        "### 2.1 - Specify the network structure \n",
        "    - n_x: input layer size\n",
        "    - n_h: #neurons in  hidden layer (hard code a value, say 10) \n",
        "    - n_y: the size of the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOsdX_bPoXCx"
      },
      "outputs": [],
      "source": [
        "def model_architecture(X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- input dataset of shape (input size, number of examples)\n",
        "    Y -- labels of shape (output size, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    n_x -- the size of the input layer\n",
        "    n_h -- the size of the hidden layer\n",
        "    n_y -- the size of the output layer\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### \n",
        "    n_x = X.shape[0] # size of input layer\n",
        "    n_h = 3\n",
        "    n_y = Y.shape[0] # size of output layer\n",
        "    ### END CODE HERE ###\n",
        "    return (n_x, n_h, n_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4-FAtl-oXC3"
      },
      "source": [
        "### 2.2 - Initialize the parameters of the model\n",
        "\n",
        "- Initialize the weights matrices with random values. \n",
        "    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).\n",
        "- Initialize the bias vectors as zeros. \n",
        "    - Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV3W6uLvoXC3"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "    \n",
        "    Returns:\n",
        "    params -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(2)\n",
        "\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    W1 = np.random.randn(n_h, n_x)*0.01\n",
        "    b1 = np.zeros((n_h, 1))\n",
        "    W2 = np.random.randn(n_y, n_h)*0.01\n",
        "    b2 = np.zeros((n_y, 1))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert (W1.shape == (n_h, n_x))\n",
        "    assert (b1.shape == (n_h, 1))\n",
        "    assert (W2.shape == (n_y, n_h))\n",
        "    assert (b2.shape == (n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBRbnqYkoXC8"
      },
      "source": [
        "### 2.3 - The Loop ####\n",
        "\n",
        "**Instructions**:\n",
        "- Look above at the mathematical representation of your classifier.\n",
        "- Define the function `sigmoid()`.\n",
        "- You can use the function `np.tanh()`. It is part of the numpy library.\n",
        "- The steps you have to implement are:\n",
        "    1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()`) by using `parameters[\"..\"]`.\n",
        "    2. Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).\n",
        "- Values needed in the backpropagation are stored in \"`cache`\". The `cache` will be given as an input to the backpropagation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4Xif_6C3FV3"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    ### Update THE CODE HERE ###\n",
        "    return 1/(1+np.exp(-x))\n",
        "    ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "AkJ4TKKSoXC9"
      },
      "outputs": [],
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m)\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "    \n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ### \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
        "    ### START CODE HERE ### \n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = np.tanh(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert(A2.shape == (1, X.shape[1]))\n",
        "    \n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    \n",
        "    return A2, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21VnrjU3oXDC"
      },
      "source": [
        "Now you have computed $A^{[2]}$ (in the Python variable \"`A2`\"), which contains $a^{[2](i)}$ for every example, you can compute the loss function as follows:\n",
        "\n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{7}$$\n",
        "\n",
        "- Implement the cross-entropy loss:\n",
        "$- \\sum\\limits_{i=0}^{m}  y^{(i)}\\log(a^{[2](i)})$:\n",
        "```python\n",
        "logprobs = np.multiply(np.log(A2),Y)\n",
        "loss = - np.sum(logprobs)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whxXuxq-oXDD"
      },
      "outputs": [],
      "source": [
        "def compute_loss(A2, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "       \n",
        "    Returns:\n",
        "    loss -- cross-entropy loss given equation (7)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1] # number of example\n",
        "\n",
        "    # Compute the cross-entropy loss\n",
        "    ### START CODE HERE ###\n",
        "    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1-Y), np.log(1 - A2))\n",
        "    loss = - np.sum(logprobs) / m\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    loss = float(np.squeeze(loss))\n",
        "\n",
        "    assert(isinstance(loss, float))\n",
        "    \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5puT2VVoXDH"
      },
      "source": [
        "Using the cache computed during forward propagation, we can now implement backward propagation.\n",
        "\n",
        "Backpropagation is usually the hardest (most mathematical) part. You can use the following six equations as vectorized implementation:\n",
        "\n",
        "$$dZ^{[2]} = A^{[2]} - Y \\tag{8}$$ \n",
        "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]}A^{[1]{T}} \\tag{9}$$ \n",
        "$$db^{[2]} = \\frac{1}{m} np.sum(dZ^{[2]}, axis = 1, keepdims = True)\\tag{10}$$ \n",
        "$$dZ^{[1]} = W^{[2]T}dZ^{[2]}*g^{[1]'}(Z^{[1]}) \\tag{11}$$ \n",
        "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]}X^{{T}} \\tag{12}$$ \n",
        "$$db^{[1]} = \\frac{1}{m} np.sum(dZ^{[1]}, axis = 1, keepdims = True)\\tag{13}$$ \n",
        "\n",
        "- $*$ denotes elementwise multiplication.\n",
        "- Notations followed:\n",
        "    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n",
        "    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n",
        "    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n",
        "    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n",
        "   \n",
        "- Tips:\n",
        "    - To compute dZ1 you'll need to compute $g^{[1]'}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$. So you can compute \n",
        "    $g^{[1]'}(Z^{[1]})$ using `(1 - np.power(A1, 2))`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHTVtf4_oXDJ"
      },
      "outputs": [],
      "source": [
        "def backprop(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters \n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data\n",
        "    Y -- \"true\" labels\n",
        "    \n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
        "    ### START CODE HERE ### \n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "    ### END CODE HERE ###\n",
        "        \n",
        "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
        "    ### START CODE HERE ### \n",
        "    A1 = cache['A1']\n",
        "    A2 = cache['A2']\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
        "    ### START CODE HERE ### \n",
        "    dZ2 = A2-Y\n",
        "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
        "    dW1 = (1/m) * np.dot(dZ1, X.T) \n",
        "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims=True)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    \n",
        "    return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsDhe51IoXDO"
      },
      "source": [
        "Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\n",
        "\n",
        "**Gradient descent rule**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter.\n",
        "\n",
        "**Illustration**: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.\n",
        "\n",
        "<img src=\"images\\sgd.gif\" style=\"width:400;height:400;\"> <img src=\"images\\sgd_bad.gif\" style=\"width:400;height:400;\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWeErPRmoXDP"
      },
      "outputs": [],
      "source": [
        "def update(parameters, grads, learning_rate = 0.01):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients \n",
        "    learning_rate -- The learning rate\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ### \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    ### START CODE HERE ### \n",
        "    dW1 = grads['dW1']\n",
        "    db1 = grads['db1']\n",
        "    dW2 = grads['dW2']\n",
        "    db2 = grads['db2']\n",
        "    ## END CODE HERE ###\n",
        "    \n",
        "    # Update rule for each parameter\n",
        "    ### START CODE HERE ### \n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSWq5OE4oXDT"
      },
      "source": [
        "### 2.4 - Integrate parts 2.1, 2.2 and 2.3 in NeuralNetwork() ####\n",
        "\n",
        "Build your neural network model in `NeuralNetwork()`.\n",
        "\n",
        "**Instructions**: The neural network model has to use the previous functions in the right order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnSyk2auoXDU"
      },
      "outputs": [],
      "source": [
        "def NeuralNetwork(X, Y, n_h, num_iterations = 10000, learning_rate = 0.01, print_loss=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset\n",
        "    Y -- labels \n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    learning_rate -- The learning rate\n",
        "    print_loss -- if True, print the loss every 1000 iterations\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to make predictions.\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    n_x = model_architecture(X, Y)[0]\n",
        "    n_y = model_architecture(X, Y)[2]\n",
        "    \n",
        "    # Initialize parameters\n",
        "    ### START CODE HERE ### \n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "         \n",
        "        ### START CODE HERE ### \n",
        "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
        "        A2, cache = forward_propagation(X, parameters)\n",
        "        \n",
        "        # loss function. Inputs: \"A2, Y, parameters\". Outputs: \"loss\".\n",
        "        loss = compute_loss(A2, Y)\n",
        " \n",
        "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
        "        grads = backprop(parameters, cache, X, Y)\n",
        " \n",
        "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
        "        parameters =  update(parameters, grads)\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Print the loss every 100 iterations\n",
        "        if print_loss and i % 100 == 0:\n",
        "            print (\"loss after iteration %i: %f\" %(i, loss))\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ud31U_ZoXDY"
      },
      "source": [
        "### 2.5 Predictions\n",
        "\n",
        "Use your model to predict by building predict().\n",
        "\n",
        "**Reminder**: $ predictions = \\begin{cases}\n",
        "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
        "      0 & \\text{otherwise}\n",
        "    \\end{cases}$  \n",
        "    \n",
        "As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: ```X_new = (X > threshold)```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPMG_zMloXDZ"
      },
      "outputs": [],
      "source": [
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    X -- input data \n",
        "    \n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model\n",
        "    \"\"\"\n",
        "    \n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    ### START CODE HERE ### \n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "    predictions = A2 > 0.5\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC3OHe-9oXDd"
      },
      "source": [
        "## 3. Model Execution\n",
        "It is time to run the model and see how it performs on the dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op5NM0gXoXDi",
        "outputId": "ce1ba1fd-89b4-43b2-f0d4-bfaf51b07081",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss after iteration 0: 0.693498\n",
            "loss after iteration 100: 0.677406\n",
            "loss after iteration 200: 0.657364\n",
            "loss after iteration 300: 0.589216\n",
            "loss after iteration 400: 0.468793\n",
            "loss after iteration 500: 0.357196\n",
            "loss after iteration 600: 0.275707\n",
            "loss after iteration 700: 0.220609\n",
            "loss after iteration 800: 0.183567\n",
            "loss after iteration 900: 0.157976\n",
            "loss after iteration 1000: 0.139656\n",
            "loss after iteration 1100: 0.126086\n",
            "loss after iteration 1200: 0.115729\n",
            "loss after iteration 1300: 0.107618\n",
            "loss after iteration 1400: 0.101124\n",
            "loss after iteration 1500: 0.095823\n",
            "loss after iteration 1600: 0.091425\n",
            "loss after iteration 1700: 0.087722\n",
            "loss after iteration 1800: 0.084565\n",
            "loss after iteration 1900: 0.081841\n",
            "loss after iteration 2000: 0.079469\n",
            "loss after iteration 2100: 0.077385\n",
            "loss after iteration 2200: 0.075540\n",
            "loss after iteration 2300: 0.073894\n",
            "loss after iteration 2400: 0.072417\n",
            "loss after iteration 2500: 0.071084\n",
            "loss after iteration 2600: 0.069875\n",
            "loss after iteration 2700: 0.068773\n",
            "loss after iteration 2800: 0.067764\n",
            "loss after iteration 2900: 0.066837\n",
            "loss after iteration 3000: 0.065981\n",
            "loss after iteration 3100: 0.065189\n",
            "loss after iteration 3200: 0.064454\n",
            "loss after iteration 3300: 0.063769\n",
            "loss after iteration 3400: 0.063129\n",
            "loss after iteration 3500: 0.062529\n",
            "loss after iteration 3600: 0.061966\n",
            "loss after iteration 3700: 0.061436\n",
            "loss after iteration 3800: 0.060936\n",
            "loss after iteration 3900: 0.060463\n",
            "loss after iteration 4000: 0.060015\n",
            "loss after iteration 4100: 0.059589\n",
            "loss after iteration 4200: 0.059185\n",
            "loss after iteration 4300: 0.058800\n",
            "loss after iteration 4400: 0.058432\n",
            "loss after iteration 4500: 0.058081\n",
            "loss after iteration 4600: 0.057745\n",
            "loss after iteration 4700: 0.057422\n",
            "loss after iteration 4800: 0.057113\n",
            "loss after iteration 4900: 0.056816\n",
            "loss after iteration 5000: 0.056529\n",
            "loss after iteration 5100: 0.056254\n",
            "loss after iteration 5200: 0.055988\n",
            "loss after iteration 5300: 0.055731\n",
            "loss after iteration 5400: 0.055483\n",
            "loss after iteration 5500: 0.055243\n",
            "loss after iteration 5600: 0.055010\n",
            "loss after iteration 5700: 0.054785\n",
            "loss after iteration 5800: 0.054566\n",
            "loss after iteration 5900: 0.054354\n",
            "loss after iteration 6000: 0.054148\n",
            "loss after iteration 6100: 0.053947\n",
            "loss after iteration 6200: 0.053752\n",
            "loss after iteration 6300: 0.053562\n",
            "loss after iteration 6400: 0.053377\n",
            "loss after iteration 6500: 0.053196\n",
            "loss after iteration 6600: 0.053020\n",
            "loss after iteration 6700: 0.052847\n",
            "loss after iteration 6800: 0.052679\n",
            "loss after iteration 6900: 0.052514\n",
            "loss after iteration 7000: 0.052353\n",
            "loss after iteration 7100: 0.052195\n",
            "loss after iteration 7200: 0.052041\n",
            "loss after iteration 7300: 0.051889\n",
            "loss after iteration 7400: 0.051740\n",
            "loss after iteration 7500: 0.051594\n",
            "loss after iteration 7600: 0.051450\n",
            "loss after iteration 7700: 0.051308\n",
            "loss after iteration 7800: 0.051169\n",
            "loss after iteration 7900: 0.051032\n",
            "loss after iteration 8000: 0.050896\n",
            "loss after iteration 8100: 0.050762\n",
            "loss after iteration 8200: 0.050630\n",
            "loss after iteration 8300: 0.050499\n",
            "loss after iteration 8400: 0.050369\n",
            "loss after iteration 8500: 0.050241\n",
            "loss after iteration 8600: 0.050113\n",
            "loss after iteration 8700: 0.049987\n",
            "loss after iteration 8800: 0.049861\n",
            "loss after iteration 8900: 0.049736\n",
            "loss after iteration 9000: 0.049612\n",
            "loss after iteration 9100: 0.049488\n",
            "loss after iteration 9200: 0.049365\n",
            "loss after iteration 9300: 0.049242\n",
            "loss after iteration 9400: 0.049119\n",
            "loss after iteration 9500: 0.048996\n",
            "loss after iteration 9600: 0.048874\n",
            "loss after iteration 9700: 0.048751\n",
            "loss after iteration 9800: 0.048628\n",
            "loss after iteration 9900: 0.048505\n"
          ]
        }
      ],
      "source": [
        "# Build a model with a n_h-dimensional hidden layer\n",
        "parameters = NeuralNetwork(X, Y, n_h = 4, num_iterations = 10000, print_loss=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8vFWiwz3FWA"
      },
      "source": [
        "## Check the accuracy on the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUdvhN9BoXDn",
        "outputId": "6fee4784-d5e3-42b4-93a2-cc2eb771ec8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.989010989010989\n"
          ]
        }
      ],
      "source": [
        "# Print accuracy\n",
        "predictions = predict(parameters, X)\n",
        "print(accuracy_score(Y.T, predictions.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjg3DINw3FWB"
      },
      "source": [
        "## Check the accuracy on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-pHP3ts3FWB",
        "outputId": "c92be46d-cda4-458a-89e4-af600b8fc51a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9736842105263158\n"
          ]
        }
      ],
      "source": [
        "predictions_test = predict(parameters, testx) \n",
        "print(accuracy_score(testy.T, predictions_test.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrT2xrXeoXDz"
      },
      "source": [
        "## Run the model multiple times with different hyperparameters and write your interpretation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72jTqgYDe1vI"
      },
      "source": [
        "Write your interpretation here\n",
        "\n",
        "1-The model has been run with different hyperparameters as follows:\n",
        " - Iterations number = 10000, resulted in: traing accuracy =  0.989010989010989 and testing accuracy = 0.9736842105263158\n",
        "\n",
        " - Iterations number = 1000, resulted in: traing accuracy =  0.978021978021978 and testing accuracy = 0.956140350877193 \n",
        "\n",
        " - Iterations number = 100, resulted in: traing accuracy =  0.6373626373626373 and testing accuracy = 0.5877192982456141\n",
        "\n",
        "\n",
        " - Hidden layer = 4, resulted in: traing accuracy =  0.989010989010989 and testing accuracy = 0.9736842105263158\n",
        "\n",
        " - Hidden layer = 10, resulted in: traing accuracy =  0.989010989010989 and testing accuracy = 0.9649122807017544\n",
        "\n",
        " - Hidden layer = 50, resulted in: traing accuracy =  0.9912087912087912 and testing accuracy = 0.9649122807017544\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66OLHmwvoXD0"
      },
      "source": [
        "What happens when you change the tanh activation for a sigmoid activation or a ReLU activation?\n",
        "\n",
        " - As the first experiment with 1000 iterations showed that with using tanh activation function, the loss reach to: 0.157976, with training accuracy reach to: 0.978021978021978, and the testing accuracy reach to: 0.956140350877193 \n",
        "\n",
        " -The experiment using sigmoid activation function with 1000 iterations showed that, the loss was a little bit heigher to reach to: 0.341950, with training accuracy dropped a little bit to reach to: 0.9384615384615385, as well as testing accuracy to reach to:0.9298245614035088\n",
        "\n",
        " - The last experiment with 1000 iterations showed that with using relu activation function, the loss reach to: 0.338804, with training accuracy dropped a little bit to reach to: 0.9472527472527472, as well as testing accuracy to each to: 0.9210526315789473\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2csauEkooXD2"
      },
      "source": [
        " ## B. Neural Network: MultiClass Classification "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkgI-4O83FWC"
      },
      "source": [
        "Modify the previous architecture to model multi-class classification task. Test your architecture on the **Statlog (Vehicle Silhouettes)** Data Set ('Vehicles.csv'). Save your solution as a seperate notebook file with appropriate filename.\n",
        "\n",
        "**Note:**\n",
        "1. Perform the train/validate/test split as 70/15/15.\n",
        "2. Use Random seed as '777' wherever needed.\n",
        "3. Report appropriate measures in addition to accuracy and also plot the confusion matrix.\n",
        "\n",
        "More details on the dataset can be found at: https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6Bo1sU2fWVM",
        "outputId": "653cc86c-aca8-4671-89bb-0336caded243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss after iteration 0: 1.386366\n",
            "loss after iteration 100: 1.386218\n",
            "loss after iteration 200: 1.385995\n",
            "loss after iteration 300: 1.385304\n",
            "loss after iteration 400: 1.383247\n",
            "loss after iteration 500: 1.377576\n",
            "loss after iteration 600: 1.363864\n",
            "loss after iteration 700: 1.337624\n",
            "loss after iteration 800: 1.299827\n",
            "loss after iteration 900: 1.256504\n",
            "loss after iteration 1000: 1.212583\n",
            "loss after iteration 1100: 1.169717\n",
            "loss after iteration 1200: 1.127703\n",
            "loss after iteration 1300: 1.086121\n",
            "loss after iteration 1400: 1.044965\n",
            "loss after iteration 1500: 1.004199\n",
            "loss after iteration 1600: 0.963538\n",
            "loss after iteration 1700: 0.922401\n",
            "loss after iteration 1800: 0.879719\n",
            "loss after iteration 1900: 0.834718\n",
            "loss after iteration 2000: 0.788729\n",
            "loss after iteration 2100: 0.744388\n",
            "loss after iteration 2200: 0.703631\n",
            "loss after iteration 2300: 0.667270\n",
            "loss after iteration 2400: 0.635353\n",
            "loss after iteration 2500: 0.607523\n",
            "loss after iteration 2600: 0.583272\n",
            "loss after iteration 2700: 0.562077\n",
            "loss after iteration 2800: 0.543459\n",
            "loss after iteration 2900: 0.527010\n",
            "loss after iteration 3000: 0.512391\n",
            "loss after iteration 3100: 0.499323\n",
            "loss after iteration 3200: 0.487581\n",
            "loss after iteration 3300: 0.476980\n",
            "loss after iteration 3400: 0.467367\n",
            "loss after iteration 3500: 0.458617\n",
            "loss after iteration 3600: 0.450624\n",
            "loss after iteration 3700: 0.443300\n",
            "loss after iteration 3800: 0.436569\n",
            "loss after iteration 3900: 0.430368\n",
            "loss after iteration 4000: 0.424642\n",
            "loss after iteration 4100: 0.419343\n",
            "loss after iteration 4200: 0.414426\n",
            "loss after iteration 4300: 0.409852\n",
            "loss after iteration 4400: 0.405582\n",
            "loss after iteration 4500: 0.401582\n",
            "loss after iteration 4600: 0.397826\n",
            "loss after iteration 4700: 0.394295\n",
            "loss after iteration 4800: 0.390974\n",
            "loss after iteration 4900: 0.387854\n",
            "loss after iteration 5000: 0.384922\n",
            "loss after iteration 5100: 0.382168\n",
            "loss after iteration 5200: 0.379580\n",
            "loss after iteration 5300: 0.377146\n",
            "loss after iteration 5400: 0.374857\n",
            "loss after iteration 5500: 0.372701\n",
            "loss after iteration 5600: 0.370668\n",
            "loss after iteration 5700: 0.368750\n",
            "loss after iteration 5800: 0.366938\n",
            "loss after iteration 5900: 0.365225\n",
            "loss after iteration 6000: 0.363602\n",
            "loss after iteration 6100: 0.362064\n",
            "loss after iteration 6200: 0.360603\n",
            "loss after iteration 6300: 0.359213\n",
            "loss after iteration 6400: 0.357889\n",
            "loss after iteration 6500: 0.356625\n",
            "loss after iteration 6600: 0.355415\n",
            "loss after iteration 6700: 0.354256\n",
            "loss after iteration 6800: 0.353142\n",
            "loss after iteration 6900: 0.352071\n",
            "loss after iteration 7000: 0.351037\n",
            "loss after iteration 7100: 0.350039\n",
            "loss after iteration 7200: 0.349074\n",
            "loss after iteration 7300: 0.348139\n",
            "loss after iteration 7400: 0.347232\n",
            "loss after iteration 7500: 0.346352\n",
            "loss after iteration 7600: 0.345496\n",
            "loss after iteration 7700: 0.344663\n",
            "loss after iteration 7800: 0.343851\n",
            "loss after iteration 7900: 0.343060\n",
            "loss after iteration 8000: 0.342288\n",
            "loss after iteration 8100: 0.341533\n",
            "loss after iteration 8200: 0.340795\n",
            "loss after iteration 8300: 0.340072\n",
            "loss after iteration 8400: 0.339364\n",
            "loss after iteration 8500: 0.338668\n",
            "loss after iteration 8600: 0.337984\n",
            "loss after iteration 8700: 0.337311\n",
            "loss after iteration 8800: 0.336647\n",
            "loss after iteration 8900: 0.335991\n",
            "loss after iteration 9000: 0.335342\n",
            "loss after iteration 9100: 0.334699\n",
            "loss after iteration 9200: 0.334061\n",
            "loss after iteration 9300: 0.333426\n",
            "loss after iteration 9400: 0.332793\n",
            "loss after iteration 9500: 0.332160\n",
            "loss after iteration 9600: 0.331526\n",
            "loss after iteration 9700: 0.330890\n",
            "loss after iteration 9800: 0.330250\n",
            "loss after iteration 9900: 0.329604\n",
            "\n",
            "accuracy:\n",
            "0.722972972972973\n"
          ]
        }
      ],
      "source": [
        "# write your code here\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from scipy.special import softmax\n",
        "from sklearn.compose import ColumnTransformer \n",
        "import pandas as pd\n",
        "import requests\n",
        "import io\n",
        "import sklearn.metrics as ms\n",
        "\n",
        "# train[category_cols] = train[category_cols].apply(categorize_label, axis=0)\n",
        "\n",
        "# url = \"https://raw.githubusercontent.com/Tar-ali/NN/main/vehicle.csv\"\n",
        "# url2 = \"https://raw.githubusercontent.com/Tar-ali/NN/main/vehicle2.csv\"\n",
        "url3 = \"https://raw.githubusercontent.com/Tar-ali/NN/main/vehicle3.csv\"\n",
        "\n",
        "#column_names = [\"COMPACTNESS\", \"CIRCULARITY\", \"DISTANCE_CIRCULARITY\", \"RADIUS_RATIO\", \"PR.AXIS_ASPECT_RATIO\", \"MAX.LENGTH_ASPECT_RATIO\", \"SCATTER_RATIO\", \"ELONGATEDNESS\", \"PR.AXIS_RECTANGULARITY\", \"MAX.LENGTH_RECTANGULARITY\", \"SCALED_VARIANCE_MA\", \"SCALED_VARIANCE_MI\", \"SCALED_RADIUS_OF_GYRATION\", \"SKEWNESS_ABOUT_MA\", \"SKEWNESS_ABOUT_MI\", \"KURTOSIS_ABOUT_MI\", \"KURTOSIS_ABOUT_MA\", \"RATIO\", \"CLASS\"]\n",
        "data = pd.read_csv(url3)\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "data[\"car\"] = data[\"car\"].astype('category')\n",
        "y = enc.fit_transform(data[['car']]).toarray()\n",
        "\n",
        "#print(y)\n",
        "\n",
        "#data[\"data\"] = encoder.fit_transform(data[\"data\"])\n",
        "# ct = ColumnTransformer([(\"car\", OneHotEncoder(),[18])], remainder=\"passthrough\")\n",
        "# data2 = ct.fit_transform(data) \n",
        "#onehotencoder = OneHotEncoder(categorical_features = [18])\n",
        "#x = onehotencoder.fit_transform(data).toarray()\n",
        "\n",
        "#y = data.car.values\n",
        "# y = encoder.fit_transform(y)\n",
        "#print(data[\"car\"].unique)\n",
        "\n",
        "del data[\"car\"]\n",
        "X = data.values.astype(np.float)\n",
        "\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "\n",
        "scaler = preprocessing.StandardScaler().fit(train_data)\n",
        "train_data = scaler.transform(train_data)\n",
        "test_data = scaler.transform(test_data)\n",
        "\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.17647, random_state=42)\n",
        "\n",
        "trainx = train_data.T\n",
        "trainy = train_labels.T\n",
        "\n",
        "testx = test_data.T\n",
        "testy =test_labels.T\n",
        "\n",
        "#trainx.shape, trainy.shape, testx.shape, testy.shape\n",
        "\n",
        "X=trainx\n",
        "Y=trainy\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def model_architecture(X, Y):\n",
        "\n",
        "    n_x = X.shape[0] # size of input layer\n",
        "    n_h = 3\n",
        "    n_y = Y.shape[0] # size of output layer\n",
        "   \n",
        "    return (n_x, n_h, n_y)\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \n",
        "    np.random.seed(777)\n",
        "\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    W1 = np.random.randn(n_h, n_x)*0.01\n",
        "    b1 = np.zeros((n_h, 1))\n",
        "    W2 = np.random.randn(n_y, n_h)*0.01\n",
        "    b2 = np.zeros((n_y, 1))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert (W1.shape == (n_h, n_x))\n",
        "    assert (b1.shape == (n_h, 1))\n",
        "    assert (W2.shape == (n_y, n_h))\n",
        "    assert (b2.shape == (n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m)\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "    \n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ### \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
        "    ### START CODE HERE ### \n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = np.tanh(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = softmax(Z2, axis= 0)\n",
        "    #A2 = softmax(Z2, axis=0)\n",
        "    # global count \n",
        "    # count+=1\n",
        "    # for i in range(A2.shape[1]):\n",
        "    #   #print(A2.shape[1])\n",
        "    #   print(A2[:, i])\n",
        "    #   print(np.argmax(A2[:, i]))\n",
        "    ### END CODE HERE ###\n",
        "    # print(\"A2\", A2.shape)\n",
        "    # print(\"Z2\", Z2.shape)\n",
        "    # print(1, X.shape[1])\n",
        "    \n",
        "    # assert(A2.shape == (1, X.shape[1]))\n",
        "    \n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    \n",
        "    return A2, cache\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def backprop(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters \n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data\n",
        "    Y -- \"true\" labels\n",
        "    \n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
        "    ### START CODE HERE ### \n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "    ### END CODE HERE ###\n",
        "        \n",
        "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
        "    ### START CODE HERE ### \n",
        "    A1 = cache['A1']\n",
        "    A2 = cache['A2']\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
        "    ### START CODE HERE ### \n",
        "    dZ2 = A2-Y\n",
        "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
        "    dW1 = (1/m) * np.dot(dZ1, X.T) \n",
        "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims=True)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    \n",
        "    return grads\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def update(parameters, grads, learning_rate = 0.01):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients \n",
        "    learning_rate -- The learning rate\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ### \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    ### START CODE HERE ### \n",
        "    dW1 = grads['dW1']\n",
        "    db1 = grads['db1']\n",
        "    dW2 = grads['dW2']\n",
        "    db2 = grads['db2']\n",
        "    ## END CODE HERE ###\n",
        "    \n",
        "    # Update rule for each parameter\n",
        "    ### START CODE HERE ### \n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def compute_loss(A2,Y):\n",
        "    \"\"\"\n",
        "    X is the output from fully connected layer (num_examples x num_classes)\n",
        "    y is labels (num_examples x 1)\n",
        "    \tNote that y is not one-hot encoded vector. \n",
        "    \tIt can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
        "    \"\"\"\n",
        "    m = Y.shape[0]\n",
        "    p =  A2     #softmax(A2)\n",
        "    log_likelihood = -np.log(p[range(m),Y.argmax(axis=1)])\n",
        "    loss = np.sum(log_likelihood) / m\n",
        "    return loss\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def NeuralNetwork(X, Y, n_h, num_iterations = 10000, learning_rate = 0.01, print_loss=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset\n",
        "    Y -- labels \n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    learning_rate -- The learning rate\n",
        "    print_loss -- if True, print the loss every 1000 iterations\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to make predictions.\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(777)\n",
        "    n_x = model_architecture(X, Y)[0]\n",
        "    n_y = model_architecture(X, Y)[2]\n",
        "    \n",
        "    # Initialize parameters\n",
        "    ### START CODE HERE ### \n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "         \n",
        "        ### START CODE HERE ### \n",
        "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
        "        A2, cache = forward_propagation(X, parameters)\n",
        "        \n",
        "        # loss function. Inputs: \"A2, Y, parameters\". Outputs: \"loss\".\n",
        "        loss = compute_loss(A2, Y)\n",
        " \n",
        "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
        "        grads = backprop(parameters, cache, X, Y)\n",
        " \n",
        "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
        "        parameters =  update(parameters, grads)\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Print the loss every 100 iterations\n",
        "        if print_loss and i % 100 == 0:\n",
        "            print (\"loss after iteration %i: %f\" %(i, loss))\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    X -- input data \n",
        "    \n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "\n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    ### START CODE HERE ### \n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "    predictions =  A2 > 0.5 #np.argmax(A2, axis=1)\n",
        "    #print(A2[:, 1])\n",
        "\n",
        "    # for i in range(A.shape[1]):\n",
        "        \n",
        "    #     # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
        "  \n",
        "    #     if(A[0,i]< 0.5):\n",
        "    #         Y_prediction[0,i] = 0\n",
        "            \n",
        "    #     else:\n",
        "    #         Y_prediction[0,i] = 1\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "parameters = NeuralNetwork(X, Y, n_h = 4, num_iterations = 10000, print_loss=True)\n",
        "forward_propagation(X, parameters)\n",
        "\n",
        "print()\n",
        "print(\"accuracy:\")\n",
        "predictions = predict(parameters, X)\n",
        "print(accuracy_score(Y.T, predictions.T))\n",
        "\n",
        "\n",
        "\n",
        "#print(train_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk5_tXzPve3k",
        "outputId": "fc4a4e5e-2664-4384-a7ad-103cf0b56e97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8031496062992126\n",
            "[[38  0  7  1]\n",
            " [ 0 15  8  0]\n",
            " [ 0  5 21  0]\n",
            " [ 0  1  1 30]]\n"
          ]
        }
      ],
      "source": [
        "predictions_test = predict(parameters, testx) \n",
        "print(accuracy_score(testy.T, predictions_test.T))\n",
        "\n",
        "labelP = np.argmax(predictions_test.T, axis = 1)\n",
        "labelR = np.argmax(testy.T, axis = 1)\n",
        "\n",
        "\n",
        "print(ms.confusion_matrix(labelP,labelR))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvoglA35oXD3"
      },
      "source": [
        "References:\n",
        "- https://www.coursera.org/learn/neural-networks-deep-learning\n",
        "- http://scs.ryerson.ca/~aharley/neural-networks/\n",
        "- http://cs231n.github.io/neural-networks-case-study/\n",
        "- https://archive.ics.uci.edu/ml/datasets.php"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD34rdqo3FWE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Neural_Network_From_Scratch.ipynb",
      "provenance": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "wRuwL",
      "launcher_item_id": "NI888"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
